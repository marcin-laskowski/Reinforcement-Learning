{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google_Dopamine_25.09.2018.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mlaskowski17/Reinforcement-Learning/blob/master/1.%20Markov%20Decision%20Processes/Google_Dopamine.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "U-XPRypXuxs0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GOOGLE DOPAMINE\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m0S-41qtu9ij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Install necessary packages.\n",
        "\n",
        "# dopamine library for RL\n",
        "!pip install --upgrade --no-cache-dir dopamine-rl\n",
        "# one of opamine dependencies\n",
        "!pip install cmake\n",
        "# Arcade Learning Environment\n",
        "!pip install atari_py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-huwaDvPBai_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Necessary imports and globals.\n",
        "\n",
        "# matrix math\n",
        "import numpy as np\n",
        "# load files\n",
        "import os\n",
        "# dopamine framework (DQN for baselines)\n",
        "from dopamine.agents.dqn import dqn_agent\n",
        "# high level agent-environment excecution engine\n",
        "from dopamine.atari import run_experiment\n",
        "# visualization + data downloading\n",
        "from dopamine.colab import utils as colab_utils\n",
        "# warnings\n",
        "from absl import flags\n",
        "\n",
        "#w here to store training logs\n",
        "BASE_PATH = '/tmp/colab_dope_run'  # @param\n",
        "# which arcade environment?\n",
        "GAME = 'Asterix'  # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXtfsrGRCfS-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "outputId": "66b78f39-6c67-4095-92e4-54305d5a6483"
      },
      "cell_type": "code",
      "source": [
        "# @title Create a  new agent from scratch.\n",
        "\n",
        "# tell the agent where to store log data\n",
        "LOG_PATH = os.path.join(BASE_PATH, 'basic_agent', GAME)\n",
        "\n",
        "\n",
        "# skeleton that we need for Google Dopamine \n",
        "# This is a way of defining structure, whatever our actions will be\n",
        "class BasicAgent(object):\n",
        "  \"\"\"This agent randomly selects an action and sticks to it. It will change\n",
        "  actions with probability switch_prob.\"\"\"\n",
        "  \n",
        "  def __init(self, sess, num_actions, switch_prob=0.1):\n",
        "    # tensorflow session\n",
        "    self.sess = sess\n",
        "    # how many possible actions can it take?\n",
        "    self._num_actions = num_actions\n",
        "    # probability of switching actions in the next timestep?\n",
        "    self._switch_prob = switch_prob\n",
        "    # initialize the action to take (randomly)\n",
        "    self._last_action = np.random.randint(num_actions)\n",
        "    # not debugging\n",
        "    self.eval_mode = False\n",
        "    \n",
        "  # How select an action? \n",
        "  # we define our policy here (we choose ranodm action)\n",
        "  def _choose_action(self):\n",
        "    if np.random.random() <= self._switch_prob:\n",
        "      self._last_action = np.random.randint(self._num_actions)\n",
        "    return self._last_action\n",
        "    \n",
        "    \n",
        "  #when it checkpoints during training, anything we should do?\n",
        "  def bundle_and_checkpoint(self, unused_checkpoint_dir, unused_iteration):\n",
        "    pass\n",
        "    \n",
        "  #loading from checkpoint\n",
        "  def unbundle(self, unused_checkpoint_dir, unused_checkpoint_version,\n",
        "               unused_data):\n",
        "    pass\n",
        "  \n",
        "  #first action to take\n",
        "  def begin_episode(self, unused_observation):\n",
        "    return self._choose_action()\n",
        "  \n",
        "  #cleanup\n",
        "  def end_episode(self, unused_reward):\n",
        "    pass\n",
        "  \n",
        "  \n",
        "  #we can update our policy here\n",
        "  #using the reward and observation\n",
        "  #dynamic programming, Q learning, monte carlo methods, etc.\n",
        "  def step(self, reward, observation):\n",
        "    return self._choose_action()\n",
        "  \n",
        "def create_basic_agent(sess, environment):\n",
        "  \"\"\"The Runner class will expect a function of this type to create an agent.\"\"\"\n",
        "  return BasicAgent(sess, num_actions=environment.action_space.n,\n",
        "                     switch_prob=0.2)\n",
        "\n",
        "# Create the runner class with this agent. We use very small numbers of steps\n",
        "# to terminate quickly, as this is mostly meant for demonstrating how one can\n",
        "# use the framework. We also explicitly terminate after 110 iterations (instead\n",
        "# of the standard 200) to demonstrate the plotting of partial runs.\n",
        "basic_runner = run_experiment.Runner(LOG_PATH,\n",
        "                                      create_basic_agent,\n",
        "                                      game_name=GAME,\n",
        "                                      num_iterations=200,\n",
        "                                      training_steps=10,\n",
        "                                      evaluation_steps=10,\n",
        "                                      max_steps_per_episode=100)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-368c68a4d3d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                                       \u001b[0mtraining_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                       \u001b[0mevaluation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                       max_steps_per_episode=100)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dopamine/atari/run_experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_dir, create_agent_fn, create_environment_fn, game_name, sticky_actions, checkpoint_file_prefix, logging_file_prefix, log_every_n, num_iterations, training_steps, evaluation_steps, max_steps_per_episode)\u001b[0m\n\u001b[1;32m    161\u001b[0m     self._sess = tf.Session('',\n\u001b[1;32m    162\u001b[0m                             config=tf.ConfigProto(allow_soft_placement=True))\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_agent_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-368c68a4d3d4>\u001b[0m in \u001b[0;36mcreate_basic_agent\u001b[0;34m(sess, environment)\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"The Runner class will expect a function of this type to create an agent.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   return BasicAgent(sess, num_actions=environment.action_space.n,\n\u001b[0;32m---> 57\u001b[0;31m                      switch_prob=0.2)\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Create the runner class with this agent. We use very small numbers of steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object() takes no parameters\n  In call to configurable 'Runner' (<function Runner.__init__ at 0x7f52bf5f97b8>)"
          ]
        }
      ]
    }
  ]
}